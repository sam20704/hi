# Digitrase

An **agentic validation layer** for document tampering detection.

This system sits on top of deterministic document scoring modules and adds
**AI-powered reasoning, contradiction detection, and forensic explanation**
without replacing the deterministic rules.

```
Deterministic Scoring â†’ Agentic Reasoning â†’ Forensic Verdict
     (rules)              (Llama + Claude)     (explanation)
```

---

## ðŸ“– Table of Contents

- [What This Does](#what-this-does)
- [What This Does NOT Do](#what-this-does-not-do)
- [Architecture](#architecture)
- [Complete Flow Visualization](#complete-flow-visualization)
- [Project Structure](#project-structure)
- [File-by-File Explanation](#file-by-file-explanation)
- [Setup](#setup)
- [Running the API](#running-the-api)
- [API Usage](#api-usage)
- [Running Tests](#running-tests)
- [Configuration Reference](#configuration-reference)
- [Error Handling](#error-handling)
- [Design Decisions](#design-decisions)

---

## What This Does

You already have deterministic modules that analyze documents:

| Module         | What It Detects                          |
|----------------|------------------------------------------|
| **Metadata**   | Editing tools, date mismatches, authors  |
| **Font**       | Mixed fonts, embedding issues, glyphs    |
| **Compression**| Double compression, quality shifts       |
| **QR**         | QR code anomalies (optional)             |

Each module produces a **score (0â€“10)** and a list of **signals**.

These scores are combined:

```
deterministic_score = min(metadata + font + compression, 10)

Priority:
  score >= 8  â†’ High
  5 <= score < 8 â†’ Medium
  score < 5   â†’ Low
```

**This API adds an intelligent layer on top:**

âœ… Audits whether module scores are consistent with their signals
âœ… Detects contradictions between modules
âœ… Identifies reinforcing evidence patterns
âœ… Produces a forensic explanation suitable for audit reports
âœ… Flags edge cases for human review

---

## What This Does NOT Do

âŒ **Does NOT use AI for detection** â€” detection is deterministic
âŒ **Does NOT override scores** â€” LLM never changes a module score
âŒ **Does NOT process images** â€” input is structured JSON only
âŒ **Does NOT use ML classification** â€” no training, no models
âŒ **Does NOT expose chain-of-thought** â€” only summarized reasoning

The LLM **reasons and explains**. It does not **detect or score**.

---

## Architecture

Two-agent pipeline:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Your System                          â”‚
â”‚                                                         â”‚
â”‚  Document â†’ Metadata Module â”€â”€â”                         â”‚
â”‚           â†’ Font Module â”€â”€â”€â”€â”€â”€â”¤                         â”‚
â”‚           â†’ Compression Moduleâ”¤                         â”‚
â”‚           â†’ QR Module (opt) â”€â”€â”˜                         â”‚
â”‚                    â”‚                                    â”‚
â”‚                    â–¼                                    â”‚
â”‚           Deterministic Score                           â”‚
â”‚           + Priority + Signals                          â”‚
â”‚                    â”‚                                    â”‚
â”‚                    â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚         FORENSIC VALIDATION API                 â”‚   â”‚
â”‚  â”‚                (this project)                    â”‚   â”‚
â”‚  â”‚                                                  â”‚   â”‚
â”‚  â”‚         JSON Case File                          â”‚   â”‚
â”‚  â”‚              â”‚                                   â”‚   â”‚
â”‚  â”‚              â–¼                                   â”‚   â”‚
â”‚  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚   â”‚
â”‚  â”‚     â”‚ Critic Agent â”‚  (Llama 3.2)               â”‚   â”‚
â”‚  â”‚     â”‚              â”‚                            â”‚   â”‚
â”‚  â”‚     â”‚ â€¢ Checks rule consistency                 â”‚   â”‚
â”‚  â”‚     â”‚ â€¢ Finds contradictions                    â”‚   â”‚
â”‚  â”‚     â”‚ â€¢ Identifies reinforcement                â”‚   â”‚
â”‚  â”‚     â”‚ â€¢ Assesses confidence                     â”‚   â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚   â”‚
â”‚  â”‚            â”‚                                     â”‚   â”‚
â”‚  â”‚            â–¼                                     â”‚   â”‚
â”‚  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚   â”‚
â”‚  â”‚     â”‚ Reflection Agent â”‚  (Claude)              â”‚   â”‚
â”‚  â”‚     â”‚                  â”‚                        â”‚   â”‚
â”‚  â”‚     â”‚ â€¢ Final tampered/not decision             â”‚   â”‚
â”‚  â”‚     â”‚ â€¢ Forensic explanation                    â”‚   â”‚
â”‚  â”‚     â”‚ â€¢ Evidence summary                        â”‚   â”‚
â”‚  â”‚     â”‚ â€¢ Human review flagging                   â”‚   â”‚
â”‚  â”‚     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚   â”‚
â”‚  â”‚            â”‚                                     â”‚   â”‚
â”‚  â”‚            â–¼                                     â”‚   â”‚
â”‚  â”‚     Forensic Verdict                            â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Agent Roles

| Agent | Model | Role | Does | Does NOT |
|-------|-------|------|------|----------|
| **Critic** | Llama 3.2 | Internal auditor | Checks consistency, finds contradictions, measures confidence | Classify, score, or judge |
| **Reflection** | Claude | Forensic judge | Produces verdict, explanation, evidence trace | Rescore, override rules, or invent evidence |

---

## Complete Flow Visualization

This is what happens from the moment you send a request to the moment you get a response:

```
                         POST /validate
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  FastAPI Endpoint  â”‚
                    â”‚   (main.py)        â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Pydantic Schema   â”‚    â† Validates input JSON
                    â”‚  Validation        â”‚    â† Rejects bad input (422)
                    â”‚  (evidence.py)     â”‚    â† Enforces scoreâ†”priority
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  ForensicValidator â”‚    â† Orchestrator
                    â”‚  (validator.py)    â”‚    â† Chains both agents
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚          STAGE 1: CRITIC         â”‚
            â”‚                                  â”‚
            â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
            â”‚    â”‚   Preflight Check      â”‚    â”‚  â† Deterministic
            â”‚    â”‚                        â”‚    â”‚  â† No LLM needed
            â”‚    â”‚ â€¢ Score math correct?  â”‚    â”‚
            â”‚    â”‚ â€¢ High score + 0 sigs? â”‚    â”‚
            â”‚    â”‚ â€¢ 0 score + many sigs? â”‚    â”‚
            â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
            â”‚                â”‚                 â”‚
            â”‚                â–¼                 â”‚
            â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
            â”‚    â”‚   Build Prompts        â”‚    â”‚
            â”‚    â”‚                        â”‚    â”‚
            â”‚    â”‚ â€¢ Load critic_prompt   â”‚    â”‚
            â”‚    â”‚ â€¢ Inject few-shots     â”‚    â”‚
            â”‚    â”‚   (if available)       â”‚    â”‚
            â”‚    â”‚ â€¢ Serialize case JSON  â”‚    â”‚
            â”‚    â”‚ â€¢ Attach warnings      â”‚    â”‚
            â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
            â”‚                â”‚                 â”‚
            â”‚                â–¼                 â”‚
            â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
            â”‚    â”‚   Llama 3.2 API Call   â”‚    â”‚
            â”‚    â”‚   (llm_clients.py)     â”‚    â”‚
            â”‚    â”‚                        â”‚    â”‚
            â”‚    â”‚ â€¢ Schema injected      â”‚    â”‚
            â”‚    â”‚ â€¢ JSON mode enforced   â”‚    â”‚
            â”‚    â”‚ â€¢ Self-healing retry   â”‚    â”‚
            â”‚    â”‚   (up to 3 attempts)   â”‚    â”‚
            â”‚    â”‚ â€¢ Response validated   â”‚    â”‚
            â”‚    â”‚   against CriticReport â”‚    â”‚
            â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
            â”‚                â”‚                 â”‚
            â”‚                â–¼                 â”‚
            â”‚         CriticReport             â”‚
            â”‚    â€¢ rule_consistency: bool       â”‚
            â”‚    â€¢ contradictions: [...]        â”‚
            â”‚    â€¢ reinforcement: [...]         â”‚
            â”‚    â€¢ confidence: 0.0â€“1.0         â”‚
            â”‚    â€¢ rerun_recommended: bool      â”‚
            â”‚                                  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚       STAGE 2: REFLECTION        â”‚
            â”‚                                  â”‚
            â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
            â”‚    â”‚   Pre-Verdict Check    â”‚    â”‚  â† Deterministic
            â”‚    â”‚                        â”‚    â”‚
            â”‚    â”‚ â€¢ Critic confidence    â”‚    â”‚
            â”‚    â”‚   < 0.5?              â”‚    â”‚
            â”‚    â”‚ â€¢ High contradictions? â”‚    â”‚
            â”‚    â”‚ â€¢ Borderline score?    â”‚    â”‚
            â”‚    â”‚ â€¢ Sparse evidence?     â”‚    â”‚
            â”‚    â”‚ â€¢ Rerun recommended?   â”‚    â”‚
            â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
            â”‚                â”‚                 â”‚
            â”‚                â–¼                 â”‚
            â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
            â”‚    â”‚   Build Prompts        â”‚    â”‚
            â”‚    â”‚                        â”‚    â”‚
            â”‚    â”‚ â€¢ Load reflection_promptâ”‚   â”‚
            â”‚    â”‚ â€¢ Combine case file    â”‚    â”‚
            â”‚    â”‚   + critic report      â”‚    â”‚
            â”‚    â”‚ â€¢ Attach review hints  â”‚    â”‚
            â”‚    â”‚ â€¢ Inject constraints:  â”‚    â”‚
            â”‚    â”‚   case_id, score,      â”‚    â”‚
            â”‚    â”‚   severity MUST match  â”‚    â”‚
            â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
            â”‚                â”‚                 â”‚
            â”‚                â–¼                 â”‚
            â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
            â”‚    â”‚   Claude API Call      â”‚    â”‚
            â”‚    â”‚   (llm_clients.py)     â”‚    â”‚
            â”‚    â”‚                        â”‚    â”‚
            â”‚    â”‚ â€¢ Schema injected      â”‚    â”‚
            â”‚    â”‚ â€¢ No CoT instruction   â”‚    â”‚
            â”‚    â”‚ â€¢ Self-healing retry   â”‚    â”‚
            â”‚    â”‚ â€¢ Response validated   â”‚    â”‚
            â”‚    â”‚   against              â”‚    â”‚
            â”‚    â”‚   ForensicVerdict      â”‚    â”‚
            â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
            â”‚                â”‚                 â”‚
            â”‚                â–¼                 â”‚
            â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
            â”‚    â”‚   Post-Verdict         â”‚    â”‚  â† Deterministic
            â”‚    â”‚   Enforcement          â”‚    â”‚  â† Safety net
            â”‚    â”‚                        â”‚    â”‚
            â”‚    â”‚ â€¢ Fix score if LLM     â”‚    â”‚
            â”‚    â”‚   changed it           â”‚    â”‚
            â”‚    â”‚ â€¢ Fix severity if LLM  â”‚    â”‚
            â”‚    â”‚   changed it           â”‚    â”‚
            â”‚    â”‚ â€¢ Fix case_id if LLM   â”‚    â”‚
            â”‚    â”‚   changed it           â”‚    â”‚
            â”‚    â”‚ â€¢ Force review flag    â”‚    â”‚
            â”‚    â”‚   if pre-check said so â”‚    â”‚
            â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
            â”‚                â”‚                 â”‚
            â”‚                â–¼                 â”‚
            â”‚         ForensicVerdict           â”‚
            â”‚    â€¢ tampered: true/false         â”‚
            â”‚    â€¢ severity: High/Medium/Low    â”‚
            â”‚    â€¢ explanation: "..."           â”‚
            â”‚    â€¢ evidence: [{source, finding}]â”‚
            â”‚    â€¢ flagged_for_human_review     â”‚
            â”‚                                  â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  PipelineResult    â”‚
                    â”‚                   â”‚
                    â”‚ â€¢ verdict          â”‚
                    â”‚ â€¢ critic_report    â”‚
                    â”‚ â€¢ duration_ms      â”‚
                    â”‚ â€¢ success          â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  JSON Response     â”‚ â†’ 200 OK
                    â”‚  (to caller)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Protection Layers

The system has **5 layers** preventing incorrect output:

| Layer | Type | Where | Purpose |
|-------|------|-------|---------|
| 1. Input Schema | Pydantic | `evidence.py` | Reject invalid input, enforce scoreâ†”priority |
| 2. Preflight Check | Deterministic | `critic_agent.py` | Catch obvious issues before LLM call |
| 3. Pre-Verdict Check | Deterministic | `reflection_agent.py` | Flag edge cases before LLM judgment |
| 4. Schema Enforcement | Pydantic | `llm_clients.py` | Reject malformed LLM output, self-heal |
| 5. Post-Verdict Enforcement | Deterministic | `reflection_agent.py` | Correct any values LLM got wrong |

**The LLM can never override deterministic rules. Ever.**

---

## Project Structure

```
forensic-validation-api/
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                      # FastAPI application + endpoints
â”‚   â”œâ”€â”€ config.py                    # Environment configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ critic_agent.py          # Critic Agent (Llama 3.2)
â”‚   â”‚   â””â”€â”€ reflection_agent.py      # Reflection Agent (Claude)
â”‚   â”‚
â”‚   â”œâ”€â”€ prompts/
â”‚   â”‚   â”œâ”€â”€ critic_prompt.txt        # System prompt for Critic
â”‚   â”‚   â”œâ”€â”€ reflection_prompt.txt    # System prompt for Reflection
â”‚   â”‚   â””â”€â”€ fewshot_examples.json    # Optional few-shot examples
â”‚   â”‚
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ evidence.py              # Input case file schema
â”‚   â”‚   â”œâ”€â”€ critic.py                # Critic report schema
â”‚   â”‚   â””â”€â”€ reflection.py            # Final verdict schema
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ llm_clients.py           # Llama + Claude API clients
â”‚   â”‚   â””â”€â”€ validator.py             # Pipeline orchestrator
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ logger.py                # Logging utilities
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_pipeline.py             # 49 tests â€” full coverage
â”‚
â”œâ”€â”€ .env                             # Your API keys (NEVER commit)
â”œâ”€â”€ .env.example                     # Template for .env
â”œâ”€â”€ .gitignore                       # Git ignore rules
â”œâ”€â”€ requirements.txt                 # Python dependencies
â””â”€â”€ README.md                        # This file
```

---

## File-by-File Explanation

### ðŸ”§ Configuration

| File | Purpose |
|------|---------|
| `config.py` | Loads all settings from `.env` using Pydantic Settings. Contains API keys, model names, retry limits, timeouts. Single source of truth for all configuration. Never hardcodes secrets. |
| `.env` | Your actual API keys and settings. **Never committed to git.** Created by copying `.env.example`. |
| `.env.example` | Template showing all available environment variables with descriptions. Safe to commit. |

### ðŸ“ Schemas (Data Contracts)

These define the **exact shape** of data flowing through the system. Every field is validated. Extra fields are rejected. Impossible states are prevented.

| File | Purpose |
|------|---------|
| `schemas/evidence.py` | Defines `EvidenceCaseFile` â€” the input to the pipeline. Contains module results (score + signals), deterministic score, and priority. **Enforces that priority matches score** (score 9 + priority "Low" is rejected). Also defines `ModuleResult` used by each detection module. |
| `schemas/critic.py` | Defines `CriticReport` â€” the output of the Critic Agent. Contains rule consistency check, list of contradictions (typed by category and impact), reinforcing patterns, confidence level (float + enum + reason), and rerun recommendation. **Enforces that confidence float matches confidence level** (0.95 with "low" is rejected). |
| `schemas/reflection.py` | Defines `ForensicVerdict` â€” the final output. Contains tampered decision, severity, explanation, structured evidence list (each item has source + finding + weight), confidence, and human review flag. **Enforces minimum 1 evidence item.** No chain-of-thought. No internal reasoning. |

### ðŸ¤– Agents

| File | Purpose |
|------|---------|
| `agents/critic_agent.py` | **The Critic** â€” an internal evidence auditor powered by Llama 3.2. Receives a case file, runs preflight checks (deterministic sanity validation), loads the system prompt (with optional few-shot examples), calls Llama, and returns a validated `CriticReport`. Does NOT classify or score. Only audits whether the deterministic scores are consistent with their signals. Includes `PromptLoader` (caches prompts from disk), `preflight_check()` (catches score math errors before LLM call), and `serialize_case_file()` (clean JSON for LLM input). |
| `agents/reflection_agent.py` | **The Reflection Agent** â€” the final forensic judge powered by Claude. Receives the case file AND the critic's report, runs pre-verdict checks (should this case be flagged for human review?), calls Claude for the final verdict, then runs post-verdict enforcement (corrects any values the LLM got wrong). Includes `should_flag_for_review()` (deterministic edge case detection) and `validate_verdict()` (the safety net that ensures LLM never overrides deterministic rules). |

### ðŸ”Œ Services

| File | Purpose |
|------|---------|
| `services/llm_clients.py` | **Production-grade LLM API clients** for both Llama (via OpenAI-compatible API) and Claude (via Anthropic API). Provides: `BaseLLMClient` (abstract base with schema-enforced generation + self-healing retry), `LlamaClient` (Critic's brain), `ClaudeClient` (Reflection's brain), `extract_json()` (robust JSON extraction from LLM responses â€” handles markdown wrapping, embedded JSON, nested braces). Classifies errors as retryable (rate limit, timeout) vs fatal (auth, bad model). Self-healing means: if the LLM returns bad JSON, the error is fed back to the LLM on retry so it can correct itself. Singleton factory functions (`get_critic_client()`, `get_reflection_client()`) ensure connection pools are reused. |
| `services/validator.py` | **The Orchestrator** â€” single entry point for the entire pipeline. `ForensicValidator.validate()` chains Critic â†’ Reflection and returns a `PipelineResult` containing the verdict, critic report, timing, and success status. Handles all errors and classifies them by stage (critic vs reflection). `PipelineResult.to_dict()` serializes everything for the API response. Convenience function `validate_case()` provides a one-liner for simple usage. |

### ðŸŒ API Layer

| File | Purpose |
|------|---------|
| `main.py` | **FastAPI application.** Defines the HTTP API with two endpoints: `GET /health` (health check + model info) and `POST /validate` (full forensic validation). Includes: application lifecycle management (startup logs config, shutdown closes LLM connections), request logging middleware (method, path, status, duration â€” no sensitive data), custom Pydantic validation error handler (clean error messages), and standardized error responses (422 for bad input, 502 for LLM failure, 500 for unexpected errors). |

### ðŸ“ Prompts

| File | Purpose |
|------|---------|
| `prompts/critic_prompt.txt` | System prompt for the Critic Agent. Defines the Critic's role, what it does (5 responsibilities), what it does NOT do (6 prohibitions), scoring rules reference (so it can check math), and a placeholder for few-shot examples. Written to produce consistent, structured audits. |
| `prompts/reflection_prompt.txt` | System prompt for the Reflection Agent. Defines the Reflection's role, what it receives (case file + critic report), what it does (6 responsibilities including tampered decision, severity passthrough, explanation, evidence listing, confidence, and review flagging), and what it does NOT do (7 prohibitions). Explicitly states severity MUST come from deterministic score. |
| `prompts/fewshot_examples.json` | **Optional.** Curated examples showing the Critic how to reason about different case types (clean, tampered, contradictions, edge cases). If missing, the system works without them â€” the prompt falls back to instruction-only mode. |

### ðŸ§ª Tests

| File | Purpose |
|------|---------|
| `tests/test_pipeline.py` | **49 tests** covering: schema validation (input, critic, verdict), preflight checks, pre-verdict checks, post-verdict enforcement, JSON serialization, full pipeline with mocked LLMs, error paths (critic failure, reflection failure, missing prompts), PipelineError/PipelineResult classes, and prompt loading. **All tests are offline** â€” LLM calls are mocked. Fast, deterministic, no API keys needed. |

---

## Setup

### Prerequisites

- Python 3.10+
- API key for [Together AI](https://together.ai) (or any OpenAI-compatible provider for Llama)
- API key for [Anthropic](https://anthropic.com) (for Claude)

### Installation

```bash
# Clone the repository
git clone https://github.com/your-org/forensic-validation-api.git
cd forensic-validation-api

# Create virtual environment
python -m venv venv
source venv/bin/activate        # Linux/Mac
# venv\Scripts\activate         # Windows

# Install dependencies
pip install -r requirements.txt

# Setup environment
cp .env.example .env
# Edit .env and add your API keys
```

### Verify Setup

```bash
# Run tests (no API keys needed)
pytest tests/test_pipeline.py -v

# Expected: 49 passed
```

---

## Running the API

### Development

```bash
uvicorn app.main:app --reload --port 8000
```

### Production

```bash
uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4
```

### Verify Running

```bash
curl http://localhost:8000/health
```

Expected:

```json
{
  "status": "healthy",
  "models": {
    "critic": "meta-llama/Llama-3.2-3B-Instruct",
    "reflection": "claude-sonnet-4-20250514"
  }
}
```

---

## API Usage

### `POST /validate`

Send a deterministic case file, receive a forensic verdict.

#### Request

```bash
curl -X POST http://localhost:8000/validate \
  -H "Content-Type: application/json" \
  -d '{
    "case_id": "DOC-2024-0042",
    "metadata": {
      "score": 6,
      "signals": [
        "multiple_tools_detected",
        "creation_date_mismatch",
        "producer_editor_conflict"
      ]
    },
    "font": {
      "score": 5,
      "signals": [
        "mixed_font_families",
        "font_embedding_mismatch"
      ]
    },
    "compression": {
      "score": 4,
      "signals": [
        "double_compression_detected",
        "quality_level_inconsistency"
      ]
    },
    "qr": null,
    "deterministic_score": 10,
    "priority": "High"
  }'
```

#### Successful Response (200)

```json
{
  "case_id": "DOC-2024-0042",
  "success": true,
  "duration_ms": 4523.17,
  "verdict": {
    "case_id": "DOC-2024-0042",
    "tampered": true,
    "severity": "High",
    "deterministic_score": 10,
    "confidence": 0.93,
    "confidence_level": "very_high",
    "explanation": "Document exhibits strong tampering indicators across all three detection modules. Multiple editing tools with date mismatch in metadata, mixed font families suggesting content from different sources, and double compression confirming the document was re-saved multiple times. All modules independently corroborate tampering.",
    "evidence": [
      {
        "source": "metadata",
        "finding": "Multiple tools and creation date mismatch indicate editing",
        "weight": "supporting"
      },
      {
        "source": "font",
        "finding": "Mixed font families from different sources",
        "weight": "supporting"
      },
      {
        "source": "compression",
        "finding": "Double compression confirms multiple re-saves",
        "weight": "supporting"
      }
    ],
    "critic_report_included": true,
    "flagged_for_human_review": false,
    "review_reason": null
  },
  "critic_report": {
    "rule_consistency": true,
    "contradictions": [],
    "reinforcement": [
      "Metadata multiple_tools reinforces font mixed_font_families",
      "All three modules independently indicate tampering"
    ],
    "confidence": 0.95,
    "confidence_level": "very_high",
    "confidence_reason": "Strong independent signals across all modules",
    "audit_notes": "Score of 10 is correctly capped from raw sum of 15. All modules show strong signals consistent with their scores.",
    "rerun_recommended": false
  }
}
```

#### Validation Error (422)

```json
{
  "success": false,
  "error": "validation_error",
  "detail": "body â†’ priority: Score 10 must be High priority"
}
```

#### Pipeline Error (502)

```json
{
  "success": false,
  "error": "pipeline_error",
  "case_id": "DOC-2024-0042",
  "failed_stage": "critic",
  "detail": "All 3 attempts failed for meta-llama/Llama-3.2-3B-Instruct"
}
```

### Input Field Reference

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `case_id` | string | âœ… | Unique document identifier |
| `metadata` | object | âœ… | Metadata module result |
| `metadata.score` | int (0â€“10) | âœ… | Module score |
| `metadata.signals` | string[] | âŒ | Detected signals |
| `font` | object | âœ… | Font module result |
| `font.score` | int (0â€“10) | âœ… | Module score |
| `font.signals` | string[] | âŒ | Detected signals |
| `compression` | object | âœ… | Compression module result |
| `compression.score` | int (0â€“10) | âœ… | Module score |
| `compression.signals` | string[] | âŒ | Detected signals |
| `qr` | object | âŒ | Optional QR module result |
| `deterministic_score` | int (0â€“10) | âœ… | Pre-computed combined score |
| `priority` | "Low"/"Medium"/"High" | âœ… | Must match score |

### Priority â†” Score Rules

```
deterministic_score >= 8  â†’ priority MUST be "High"
5 <= deterministic_score < 8 â†’ priority MUST be "Medium"
deterministic_score < 5   â†’ priority MUST be "Low"
```

Mismatches are rejected with a 422 error.

---

## Running Tests

```bash
# All tests
pytest tests/test_pipeline.py -v

# Specific test class
pytest tests/test_pipeline.py::TestEvidenceCaseFileSchema -v

# Specific test
pytest tests/test_pipeline.py::TestFullPipeline::test_clean_document_pipeline -v

# With print output
pytest tests/test_pipeline.py -v -s
```

### Test Categories

| Category | Tests | What It Validates |
|----------|-------|-------------------|
| Schema: Evidence | 10 | Input validation, scoreâ†”priority, boundaries |
| Schema: Critic | 4 | Confidence alignment, contradiction structure |
| Schema: Verdict | 3 | Evidence requirements, extra field rejection |
| Preflight Checks | 5 | Score math, signal count anomalies |
| Pre-Verdict Checks | 6 | Review flagging logic |
| Post-Verdict Enforcement | 5 | Score/severity/case_id correction |
| Serialization | 3 | JSON output format |
| Full Pipeline | 4 | End-to-end with mocked LLMs |
| Error Paths | 3 | Critic/reflection failures |
| PipelineError | 2 | Error class behavior |
| PipelineResult | 2 | Result serialization |
| PromptLoader | 2 | Cache and missing file handling |

**Total: 49 tests â€” all offline, no API keys needed**

---

## Configuration Reference

All settings are in `.env`. None are hardcoded.

| Variable | Default | Description |
|----------|---------|-------------|
| `LLAMA_API_KEY` | (required) | API key for Llama provider |
| `LLAMA_BASE_URL` | `https://api.together.xyz/v1` | OpenAI-compatible API base URL |
| `LLAMA_MODEL` | `meta-llama/Llama-3.2-3B-Instruct` | Model identifier |
| `LLAMA_MAX_TOKENS` | `2048` | Max response tokens |
| `LLAMA_TEMPERATURE` | `0.0` | Sampling temperature (0 = deterministic) |
| `LLAMA_TIMEOUT` | `30` | Request timeout in seconds |
| `CLAUDE_API_KEY` | (required) | Anthropic API key |
| `CLAUDE_MODEL` | `claude-sonnet-4-20250514` | Claude model version |
| `CLAUDE_MAX_TOKENS` | `2048` | Max response tokens |
| `CLAUDE_TEMPERATURE` | `0.0` | Sampling temperature |
| `CLAUDE_TIMEOUT` | `60` | Request timeout in seconds |
| `MAX_RETRIES` | `3` | Retry attempts per LLM call |
| `RETRY_BASE_DELAY` | `1.0` | Base delay for exponential backoff |
| `LOG_LEVEL` | `INFO` | Logging level |

### Supported Llama Providers

Any OpenAI-compatible API works. Change `LLAMA_BASE_URL`:

| Provider | Base URL |
|----------|----------|
| Together AI | `https://api.together.xyz/v1` |
| Groq | `https://api.groq.com/openai/v1` |
| Fireworks | `https://api.fireworks.ai/inference/v1` |
| Ollama (local) | `http://localhost:11434/v1` |
| vLLM (local) | `http://localhost:8000/v1` |

---

## Error Handling

### Error Classification

| Error Type | HTTP Code | Retried? | Example |
|------------|-----------|----------|---------|
| Validation error | 422 | No | Missing field, wrong priority |
| Rate limit | 502 | Yes (3x) | Too many API requests |
| Timeout | 502 | Yes (3x) | LLM took too long |
| Auth failure | 502 | No | Invalid API key |
| Bad JSON from LLM | â€” | Yes (self-healing) | LLM wrapped JSON in markdown |
| Schema violation from LLM | â€” | Yes (self-healing) | LLM added extra fields |
| Unexpected error | 500 | No | Bug in code |

### Self-Healing Retry

When the LLM returns invalid JSON or violates the schema:

1. The error message is extracted
2. It's appended to the prompt as a correction instruction
3. The LLM is called again with the error context
4. LLMs fix ~80% of formatting errors when told what broke

This happens transparently inside `llm_clients.py`.

---

## Design Decisions

| Decision | Why |
|----------|-----|
| **LLMs never override scores** | Forensic compliance. Deterministic rules are the source of truth. LLMs add reasoning, not judgment. |
| **Two agents, not one** | Separation of concerns. Critic audits evidence. Reflection judges. Neither does both. |
| **Temperature 0.0** | Forensic system needs reproducible outputs. No creative variation. |
| **No chain-of-thought in output** | Legal safety. CoT can contain speculative reasoning that shouldn't appear in audit reports. |
| **`extra = "forbid"` on all schemas** | Prevents LLM hallucination of fields not in the contract. |
| **Post-verdict enforcement** | Defense in depth. Even if the LLM returns wrong severity, it gets corrected before the response. |
| **Classified exceptions** | Rate limits get retried. Auth errors don't. Saves time and API costs. |
| **Singleton LLM clients** | HTTP connection pools are expensive to create. Shared across all requests. |
| **Preflight checks before LLM** | Catches obvious data errors without wasting an API call. |
| **Few-shots are optional** | System works without them. Add them later when you have curated examples. |

---

## Interactive Docs

When the server is running:

- **Swagger UI**: [http://localhost:8000/docs](http://localhost:8000/docs)
- **ReDoc**: [http://localhost:8000/redoc](http://localhost:8000/redoc)

---

## License

Internal use only. Not for redistribution.
