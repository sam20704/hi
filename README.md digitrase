# DIGITRASE - AGENT ENABLED AUTOVALIDATION

An **AI-assisted reasoning layer** for document tampering detection.

This project does **NOT** detect tampering by itself.

Instead, it sits **on top of your existing deterministic detection system** and adds:

âœ… Logical auditing of results  
âœ… Contradiction detection  
âœ… Human-readable forensic explanations  
âœ… Edge-case flagging for manual review  

Think of it as:

> **Your rules detect.  
> This system thinks.  
> Humans decide.**

---

# ğŸ§  Big Picture

You already have modules that analyze documents:

| Module | Detects |
|--------|---------|
| Metadata | Editing tools, dates, authors |
| Font | Mixed fonts, embedding |
| Compression | Re-saves, quality shifts |
| QR (optional) | QR anomalies |

Each module produces:

```

score (0â€“10)
signals (list of findings)

```

Your system then computes:

```

deterministic_score = min(metadata + font + compression, 10)
priority = Low / Medium / High

```

### That part stays exactly as-is.

---

## What this API adds

After your system finishes scoring:

1. A **Critic AI** audits your results
2. A **Reflection AI** produces a final forensic verdict

Nothing is re-scored.

Nothing is overridden.

Only reasoning is added.

---

# ğŸ§± Core Principle

### â— AI never replaces your rules.

LLMs are used ONLY to:

- Check consistency
- Explain evidence
- Detect contradictions
- Write summaries

All scoring remains deterministic.

---

# ğŸ”„ Full Flow (Simple Version)

```

Your System
â†“
JSON Case File
â†“
Critic Agent (Llama)
â†“
Reflection Agent (Claude)
â†“
Forensic Verdict

```

---

# ğŸ¤– Agent Roles

## ğŸ§ª Critic Agent (Llama 3.2)

Acts like an internal auditor.

It:

âœ… Verifies scores match signals  
âœ… Finds contradictions between modules  
âœ… Finds reinforcement patterns  
âœ… Measures confidence  
âœ… Suggests reruns if needed  

It does NOT:

âŒ Decide tampering  
âŒ Assign severity  
âŒ Modify scores  

---

## âš– Reflection Agent (Claude)

Acts like a forensic examiner.

It:

âœ… Produces tampered / not tampered  
âœ… Assigns severity (from score only)  
âœ… Writes explanation  
âœ… Lists evidence  
âœ… Flags human review  

It does NOT:

âŒ Change scores  
âŒ Invent evidence  
âŒ Override priority  

---

# ğŸ›¡ Safety Layers

Five independent protections:

| Layer | Purpose |
|------|---------|
| Input schema | Reject bad input |
| Preflight | Catch obvious errors |
| Critic audit | Logical consistency |
| Schema enforcement | Reject LLM hallucinations |
| Post-verdict fixes | Force deterministic values |

LLMs can never override deterministic rules.

---

# ğŸ“ Project Structure

```

forensic-validation-api/

app/
â”œâ”€â”€ main.py
â”œâ”€â”€ config.py

agents/
â”œâ”€â”€ critic_agent.py
â””â”€â”€ reflection_agent.py

schemas/
â”œâ”€â”€ evidence.py
â”œâ”€â”€ critic.py
â””â”€â”€ reflection.py

services/
â”œâ”€â”€ llm_clients.py
â””â”€â”€ validator.py

prompts/
â”œâ”€â”€ critic_prompt.txt
â”œâ”€â”€ reflection_prompt.txt
â””â”€â”€ fewshot_examples.json

tests/
â””â”€â”€ test_pipeline.py

````

---

# ğŸ“„ File Overview

### main.py
FastAPI server and endpoints.

### config.py
Loads environment variables.

### schemas/
Strict data contracts (input, critic output, final verdict).

### agents/
LLM reasoning logic.

### services/
LLM clients + pipeline orchestration.

### prompts/
System prompts for both agents.

### tests/
49 offline tests covering the entire pipeline.

---

# ğŸš€ Quick Start

## Install

```bash
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
cp .env.example .env
````

Add API keys to `.env`.

---

## Run Tests

```bash
pytest tests/test_pipeline.py -v
```

---

## Start API

```bash
uvicorn app.main:app --reload
```

Open:

[http://localhost:8000/docs](http://localhost:8000/docs)

---

# ğŸ“¡ Example Request

```bash
POST /validate
```

```json
{
  "case_id": "DOC1",
  "metadata": {"score":6,"signals":["tool_mismatch"]},
  "font": {"score":5,"signals":["mixed_fonts"]},
  "compression": {"score":4,"signals":["double_compression"]},
  "deterministic_score":10,
  "priority":"High"
}
```

---

# âš™ Configuration

All settings come from `.env`.

Supports:

* Together AI
* Groq
* Fireworks
* Ollama
* vLLM

---

# ğŸ§© Design Philosophy

| Principle                | Reason                  |
| ------------------------ | ----------------------- |
| Deterministic first      | Legal reliability       |
| Two agents               | Separation of reasoning |
| No chain-of-thought      | Audit safety            |
| Temperature 0            | Reproducibility         |
| Post-verdict enforcement | Defense in depth        |
| Optional few-shots       | Flexible tuning         |

---

# ğŸ Summary

This is NOT an AI detector.

This is an **AI forensic reasoning layer**.

It:

âœ” trusts your rules
âœ” audits your outputs
âœ” explains results
âœ” flags uncertainty
âœ” protects against hallucination

